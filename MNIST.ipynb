import tensorflow as tf # подключаем TF
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets("MNIST_data", one_hot=True) #загружаем гоовый датасет с базой данных MNIST (рукописные цифры)

x = tf.placeholder("float", [None, 784])                      #создаём входной placeholder, размер 784, т.к размер изображения 28*28
y = tf.placeholder("float", [None, 10])                       #создаём placeholder ответов длинны 10, т.к предсказываем вероятность опявления 1 из 10 цифр

W = tf.Variable(tf.zeros([784, 10]))                          #переменная весов, которые будут подбираться в процессе изучения
b = tf.Variable(tf.zeros([10]))                               #насколько понял - штрафное слагаемое которое мешает моделипереобучаться

linear_prediction = tf.matmul(x,W) + b                        #получив матрицу выражений x умножаем её на веса W и штрафуем произведение c помощью b (линейное преобразование)
scaled_prediction = tf.nn.softmax(linear_prediction)          #обобщение логистической функции, которая трактует оценку вероятности принадлежности тому или иному классу, значения [0, 1]



loss_function = tf.losses\
         .softmax_cross_entropy(y, linear_prediction)       
   

learning_rate = 0.1                                           #скорость спуска
optimizer = tf.train\
        .GradientDescentOptimizer(learning_rate)\
        .minimize(loss_function)                              #оптимизация с помощью минимизации функции потерь
        
init = tf.global_variables_initializer()                      #инициализация переменных                         
sess = tf.InteractiveSession()                                #задаём сессию
sess.run(init)                                                #запуск сессии (сессия позволяет освобождать, используемые ресурсы)

batch_size = 100                                              #"скармливаем" оптимизатору обучающие примеры по кусочкам размерм 100

for iteration in range(3000):
    batch_x, batch_y = mnist.train.next_batch(batch_size)     #
    sess.run(optimizer,                                       # запускаем оптимизацию
                 feed_dict={x: batch_x, y: batch_y})
    if iteration % 500 == 0:
        loss = loss_function.eval(                            #считаем ошибку
                {x: mnist.test.images, 
                 y: mnist.test.labels})
        print ("#{}, loss={:.4f}".format(iteration, loss))

 # Задаем граф вычислений, выдающий точность предсказания
predicted_label = tf.argmax(scaled_prediction, 1)             #предсказанные лейблы
actual_label = tf.argmax(y, 1)                                #истинные лейблы
is_equal_labels = tf.equal(actual_label, predicted_label)     #если лейблы равны - 1, иначе - 0
accuracy = tf.reduce_mean(tf.cast(is_equal_labels, "float"))  #усредняем

    # Вычисляем точность
accracy_value = accuracy.eval({x: mnist.test.images, y: mnist.test.labels}) #подставляем тестовые данные
print ("Accuracy:", accracy_value)

    # Предсказываем лейбы для тествого датасета
predicted_label = tf.argmax(scaled_prediction, 1)
predicted_test_values = predicted_label.eval(               #запускаем на тестовых изоражениях
        {x: mnist.test.images})
print ("Predictions:",(predicted_test_values))

tf.InteractiveSession.close(sess) 
